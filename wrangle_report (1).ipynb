{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangling report\n",
    "\n",
    "This is a report on the WeRateDogs dataset. This twitter dataset (extracted from Twitter) is the tweet archive of Twitter user @dog_rates, also known as WeRateDogs. WeRateDogs is a Twitter account that rates people's dogs with a humorous comment about the dog. These ratings almost always have a denominator of 10. The numerators, though? Almost always greater than 10. 11/10, 12/10, 13/10, etc. Why? Because \"they're good dogs Brent.\" WeRateDogs has over 4 million followers and has received international media coverage.\n",
    "\n",
    "WeRateDogs downloaded their Twitter archive and sent it to Udacity via email exclusively for us to use in this project. This archive contains basic tweet data (tweet ID, timestamp, text, etc.) for all 5000+ of their tweets as they stood on August 1, 2017.\n",
    "\n",
    "For this investigation, we specifically gathered 3 datasets for WeRateDogs tweets on dog ratings. We then wrangled and got some insights on them.\n",
    "\n",
    "This report covers the Gathering and Wrangling Phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering data\n",
    "\n",
    "Here, I have wrangled multiples files with data in common.\n",
    "\n",
    "The three files that I have generated in the gathering of the data are:\n",
    "1. WeRateDogs Twitter archive data (twitter_archive_enhanced.csv): Data in a CSV format that contains some metadata of the each tweet like the datetime and also the type of dog.\n",
    "2. Tweet image prediction (image_predictions.tsv): Data in a TSV format that contains the prediccions of breed of dog in the images of the tweets\n",
    "3. Programmatically query additional data via the Twitter API (tweet_json.txt):  Data that comes from the API of Twitter with the favorites and retweets of each tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing data\n",
    "\n",
    "After gathering all the data I have assessed the data with some commands like head(), info(), etc. and then I defined over 8 quality issues and 2 tidiness issues. This issues are the following:\n",
    "\n",
    "### Quality Issues\n",
    "\n",
    "#### For archive_df dataframe:\n",
    "\n",
    "1. Data types and Organization for the Dog types; \n",
    "   Dog types doggo, floofer, pupper, puppo should be boolean not float.Also there 4 columns should be just 1 column as they are mutually exclusive yet related. None should represent False and the respective other value as True in the doggo, floofer, pupper, puppo columns\n",
    "\n",
    "2. Ratings issues;\n",
    "    There are 20 invalid data entries in rating_denominator column with rating denominator larger than 10. Also some rows contain decimal values in rating_numerator that didn't load well.\n",
    "\n",
    "3. Unrealistic names: \n",
    "    The table has unrealistic values (discriminants) in the name column indicated as a, an, the, ... . These account for over 70 entries out of the remaining (957 -745) 223 unique values for name. The must have been read as names in error.\n",
    "\n",
    "4. Datatype issues with timestamp column: \n",
    "    It has a data type string instead of date-time\n",
    "\n",
    "#### For data dataframe:\n",
    "\n",
    "5. Data Consistency issues with tweet_id as id and timestamp as created_at column\n",
    "\n",
    "6. created_at column on the data table should be of data type datetime not string\n",
    "\n",
    "7. There are no entries in the contributors, co-ordinators, geo and place columns\n",
    "\n",
    "#### The image_df dataframe:\n",
    "\n",
    "8. Drop 66 jpg_url duplicated\n",
    "\n",
    "9. This has images that are not dogs. Also, some images indicated false actually contain images of dogs. We can also see that some image name description tell us whether or not it is a dog. Their confidence level will also tell us how confident they are that the image doesn't contain a dog. These are indicated under columns p1_dog, p1, p2_dog, p2, p3_dog, p3. Data that indicates False in all 3 needs to be removed\n",
    "\n",
    "### Tidiness issues\n",
    "\n",
    "#### For data dataframe:\n",
    "1. data table has entities, extended_entities, and user with more than on entry at a time that are not necessary for the investigation\n",
    "\n",
    "#### The merged dataframe\n",
    "\n",
    "2. All columns related to retweet as they are not necessary for this investigation\n",
    "3. Duplicate entries\n",
    "4. Some columns are unnecessary columns for this investigation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning data\n",
    "\n",
    "I used the Define-Code-Test format to fix the above tidiness and quality issues. I have merged the three dataset and stored them in a csv file, twitter_archive_master.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import call\n",
    "call(['python', '-m', 'nbconvert', 'wrangle_report.ipynb'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
